# -*- coding: utf-8 -*-
"""SVR, SGD, MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vQ5P2o844LzMMUQccfJdFhqrmZ5z9uMZ
"""

import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

galton = pd.read_csv("/content/drive/MyDrive/데베응용-오성팀/코랩 노트북/data/Galtons_Height_Data_898.csv")
fam_height = pd.read_csv("/content/drive/MyDrive/데베응용-오성팀/코랩 노트북/data/My_Family_Height_Data_67.csv")
ohsung_data = pd.read_csv("/content/drive/MyDrive/데베응용-오성팀/코랩 노트북/data/output_2_23.csv")

height_data = pd.concat([galton, fam_height, ohsung_data],ignore_index=True)
height_data.head()

height_data.describe()

galton_height = galton["Height(cm)"]

height_data = height_data.drop(['Family', 'Father', 'Mother', 'Height'], axis=1)
height_data

gender = pd.get_dummies(height_data.Gender)
male_height = height_data[height_data['Gender'] == 'M']
male_height = male_height.drop(['Gender'], axis=1)
female_height = height_data[height_data['Gender'] == 'F']
female_height = female_height.drop(['Gender'], axis=1)

male_height

gender = pd.get_dummies(height_data.Gender)
height_data = pd.concat([gender, height_data], axis=1)
height_data = height_data.drop(['Gender'], axis=1)
height_data

X = pd.DataFrame(height_data.iloc[:,:5])
y = pd.DataFrame(height_data['Height(cm)'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.model_selection import train_test_split

X_male = pd.DataFrame(male_height.iloc[:,:5])
y_male = pd.DataFrame(male_height['Height(cm)'])
X_female = pd.DataFrame(female_height.iloc[:,:5])
y_female = pd.DataFrame(female_height['Height(cm)'])

Xm_train, Xm_test, ym_train, ym_test = train_test_split(X_male, y_male, test_size=0.2, random_state=0)
Xf_train, Xf_test, yf_train, yf_test = train_test_split(X_female, y_female, test_size=0.2, random_state=0)

"""**SVR**"""

# Importing the libraries
import numpy as np

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_tdata = sc_X.fit_transform(X_train.sort_index())
# Transform y into 2D array
y_tdata = np.array(y_train.sort_index()).reshape((len(y_train), 1))
y_tdata = sc_y.fit_transform(y_tdata)

from sklearn.preprocessing import StandardScaler
sc_Xm = StandardScaler()
sc_ym = StandardScaler()
Xm_tdata = sc_Xm.fit_transform(Xm_train.sort_index())
# Transform y into 2D array
ym_tdata = np.array(ym_train.sort_index()).reshape((len(ym_train), 1))
ym_tdata = sc_ym.fit_transform(ym_tdata)

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X_tdata, y_tdata)

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(Xm_tdata, ym_tdata)

# Predicting a new result
y_pred = regressor.predict(X_tdata)
# Invert y_pred result
y_pred = sc_y.inverse_transform(y_pred.reshape(-1, 1))

ym_pred = regressor.predict(Xm_tdata)
ym_pred = sc_ym.inverse_transform(ym_pred.reshape(-1, 1))

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

score = r2_score(y_train.sort_index(), y_pred)
mse = mean_squared_error(y_train.sort_index(), y_pred)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

score = r2_score(ym_train.sort_index(), ym_pred)
mse = mean_squared_error(ym_train.sort_index(), ym_pred)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

# Test
sc_X = StandardScaler()
sc_y = StandardScaler()
X_tdata = sc_X.fit_transform(X_test.sort_index())
y_tdata = np.array(y_test.sort_index()).reshape((len(y_test), 1))
y_tdata = sc_y.fit_transform(y_tdata)
y_pred = regressor.predict(X_tdata)
y_pred = sc_y.inverse_transform(y_pred.reshape(-1, 1))

score = r2_score(y_test.sort_index(), y_pred)
mse = mean_squared_error(y_test.sort_index(), y_pred)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

sc_Xm = StandardScaler()
sc_ym = StandardScaler()
Xm_tdata = sc_Xm.fit_transform(Xm_test.sort_index())
ym_tdata = np.array(ym_test.sort_index()).reshape((len(ym_test), 1))
ym_tdata = sc_ym.fit_transform(ym_tdata)
ym_pred = regressor.predict(Xm_tdata)
ym_pred = sc_ym.inverse_transform(ym_pred.reshape(-1, 1))

score = r2_score(ym_test.sort_index(), ym_pred)
mse = mean_squared_error(ym_test.sort_index(), ym_pred)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

"""**SGD+MLP**"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler

class Dataset(torch.utils.data.Dataset):
  def __init__(self, X, y, scale_data=True):
    if not torch.is_tensor(X) and not torch.is_tensor(y):
      if scale_data:
          X = StandardScaler().fit_transform(X)
      self.X = torch.from_numpy(X)
      self.y = torch.from_numpy(y)

  def __len__(self):
      return len(self.X)

  def __getitem__(self, i):
      return self.X[i], self.y[i]

class MLP(nn.Module):
  def __init__(self):
    super().__init__()
    self.layers = nn.Sequential(
      nn.Linear(5, 64),
      nn.ReLU(),
      nn.Linear(64, 32),
      nn.ReLU(),
      nn.Linear(32, 1)
    )
  def forward(self, x):
    return self.layers(x)

torch.manual_seed(42)
mlp = MLP()
  
loss_function = nn.L1Loss()
optimizer = torch.optim.SGD(mlp.parameters(), lr=1e-4)

dataset = Dataset(X_train.sort_index().values, y_train.sort_index().values)
trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)

for epoch in range(0, 5): 
  print(f'Starting epoch {epoch+1}')
  current_loss = 0.0
  
  for i, data in enumerate(trainloader, 0):
    inputs, targets = data
    inputs, targets = inputs.float(), targets.float()
    targets = targets.reshape((targets.shape[0], 1))
    
    optimizer.zero_grad()
    outputs = mlp(inputs)
    
    loss = loss_function(outputs, targets)
    loss.backward()
    optimizer.step()
    
    current_loss += loss.item()
    if i % 10 == 0:
        print('Loss after mini-batch %5d: %.3f' %
              (i + 1, current_loss / 500))
        current_loss = 0.0
print('Training process has finished.')

sc_X = StandardScaler()
sc_y = StandardScaler()
X_train_py = sc_X.fit_transform(X_train.sort_index())

y_pred = mlp(torch.tensor(X_train_py).float())
y_train_sc = sc_y.fit_transform(y_train.sort_index())

y_pred_origin = sc_y.inverse_transform(y_pred.detach().numpy())

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
score = r2_score(y_train.sort_index(), y_pred_origin)
mse = mean_squared_error(y_train.sort_index(), y_pred_origin)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

X_test = torch.from_numpy(sc_X.fit_transform(X_test.sort_index())).float()
y_pred = mlp(torch.tensor(X_test))
y_train_sc = sc_y.fit_transform(y_test.sort_index())
y_pred_origin = sc_y.inverse_transform(y_pred.detach().numpy())

score = r2_score(y_test.sort_index(), y_pred_origin)
mse = mean_squared_error(y_test.sort_index(), y_pred_origin)
print("R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}".format(score, mse, np.sqrt(mse)))

test_dataset = Dataset(np.array(X_test), np.array(y_test))
test_dataset.X

y_pred = mlp(test_dataset.X.float())

plt.scatter(range(test_dataset.__len__()),test_dataset.y, color = 'red')
plt.scatter(range(test_dataset.__len__()),y_pred.detach().numpy(), color = 'blue')
plt.show()

plt.hist(galton_height)
plt.title("Pandas의 Plot메소드 사용 예")
plt.xlabel("시간")
plt.ylabel("Data")
plt.show()

galton = df_galton[["Family", "Gender", "Height(cm)", "Father(cm)", "mother(cm)"]]
galton

first = galton.iloc[0].to_dict()
fam = first['Family']
fafa = first['Father(cm)']
mama = first['mother(cm)']
male = []
female = []
if first['Gender'] == 'M':
    male.append(first['Height(cm)'])
else:
    female.append(first['Height(cm)'])

galton_male = pd.DataFrame(columns = ['Height(cm)', 'Father(cm)', 'mother(cm)'])
galton_female = pd.DataFrame(columns = ['Height(cm)', 'Father(cm)', 'mother(cm)'])
mi = 0
fi = 0

for i in range(1, 898):
    tmp = galton.iloc[i].to_dict()
    if fam == tmp['Family']:
        if tmp['Gender'] == 'M':
            male.append(tmp['Height(cm)'])
        else:
            female.append(tmp['Height(cm)'])
    else:
        if male:
            cm = 0
            for h in male:
                cm += h
            tmp_dict = {
                'Height(cm)' : cm/len(male),
                'Father(cm)' : fafa,
                'mother(cm)' : mama,
            }
            galton_male = galton_male.append(tmp_dict, ignore_index=True)
        if female:
            cm = 0
            for h in female:
                cm += h
                
            tmp_dict = {
                'Height(cm)' : cm/len(female),
                'Father(cm)' : fafa,
                'mother(cm)' : mama,
            }
            galton_female = galton_female.append(tmp_dict, ignore_index=True)
        
        fam = tmp['Family']
        male = []
        female = []
        if tmp['Gender'] == 'M':
            male.append(tmp['Height(cm)'])
        else:
            female.append(tmp['Height(cm)'])
        fafa = tmp['Father(cm)']
        mama = tmp['mother(cm)']

import seaborn as sns 
sns.heatmap(data = galton_male.corr(), annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

galton_male

"""# SVR (Support Vector Regresson)

ϵ : 회귀식 위아래 사용자가 지정한 값 (허용하는 노이즈 정도라고 생각하면 됨)
ξ : 튜브 밖에 벗어난 거리 (회귀식 위쪽)
ξ* : 튜브 밖에 벗어난 거리 (회귀식 아래쪽)

SVR은 데이터에 노이즈가 있다고 가정하며, 이러한 점을 고려하여 노이즈가 있는 실제 값을 완벽히 추정하는것을 추구하지 않는다. 따라서 적정 범위(2ϵ) 내에서는 실제값과 예측값의 차이를 허용!

커널 트릭을 통해 비선형 데이터를 다룰 수도 있다.
"""

import numpy as np
from sklearn import datasets
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, explained_variance_score
from sklearn.utils import shuffle

data = datasets.load_boston()
X, y = shuffle(data.data, data.target, random_state = 7)
num_training = int(0.8 * len(X))
X_train, y_train = X[:num_training], y[:num_training]
X_test, y_test = X[num_training:], y[num_training:]

# Create Support Vector Regression model
# kernel : 선형 커널
# C : 학습 오류에 대한 패널티, C 값이 클 수록 모델이 학습 데이터에 좀 더 최적화 됨, 너무 크면 오버피팅 발생
# Epsilon : 임계값, 예측한 값이 GT 범위 안에 있으면 패널티 부여 X
sv_regressor = SVR(kernel='linear', C=1.0, epsilon=0.1)

sv_regressor.fit(X_train, y_train)

y_pred = sv_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
evs = explained_variance_score(y_test, y_pred)

test_data = [3.7, 0, 18.4, 1, 0.87, 5.95, 91, 2.5052, 26, 666, 20.2, 351.34, 15.27]
print(sv_regressor.predict([test_data])[0])

galton_array_m = galton_male.values
X, y = galton_array_m[:,1:3] , galton_array_m[:,0] 

num_training = int(0.8 * len(X))
X_train, y_train = X[:num_training], y[:num_training]
X_test, y_test = X[num_training:], y[num_training:]

sv_regressor = SVR(kernel='linear', C=1.0, epsilon=0.1)

sv_regressor.fit(X_train, y_train)

y_pred = sv_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
evs = explained_variance_score(y_test, y_pred)

test_data = [180.1, 168.12]
print(sv_regressor.predict([test_data])[0])

"""# SGD
![image.png](attachment:image.png)
## 확률적 경사 하강법
1. 데이터를 미니배치 형태로 무작위 선정
2. 경사 하강법을 이용해 매개변수 갱신
3. 데이터 중 하나 추출하고 그래디언트(경사) 계산
4. 경사 하강 알고리즘 적용

장점 : local optimal에 빠질 위험이 적고 속도가 빠르다

단점 : 오차율이 커서 global optimal을 찾지 못할 수 있고 데이터를 하나씩 처리해서 GPU 효율이 나쁘다

* 최적화 함수이기 때문에 MLP 단계에서 사용할 예정

# MLP (Multi Layer Perceptron)
![image.png](attachment:image.png)
그냥 SVM에서 선을 여러개 그을 수 있다고 생각하면 됨..
"""

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
# define dataset
X, y = galton_array_m[:,1:3] , galton_array_m[:,0] 

# define model
model = Sequential()
model.add(Dense(100, activation='relu', input_dim=3))
model.add(Dense(1))
model.compile(optimizer='SGD', loss='mse')
# fit model
model.fit(X, y, epochs=2000, verbose=0)
# demonstrate prediction
x_input = array([175.3, 162.1])
x_input = x_input.reshape((1, 2))
yhat = model.predict(x_input, verbose=0)
print(yhat)

